{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3524cb73-4a40-464e-b5cc-b47c0ac8eea8",
   "metadata": {},
   "source": [
    "# NCI-Deepmind GraphCast-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20371639-fb5d-4ae7-b476-46f04f703cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffdd76ef-666c-4227-91fd-6a61286ba72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "\n",
    "import dataclasses\n",
    "import datetime\n",
    "import functools\n",
    "import math\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "from google.cloud import storage\n",
    "from graphcast import autoregressive\n",
    "from graphcast import casting\n",
    "from graphcast import checkpoint\n",
    "from graphcast import data_utils\n",
    "from graphcast import graphcast\n",
    "from graphcast import normalization\n",
    "from graphcast import rollout\n",
    "from graphcast import xarray_jax\n",
    "from graphcast import xarray_tree\n",
    "from IPython.display import HTML\n",
    "import ipywidgets as widgets\n",
    "import haiku as hk\n",
    "import jax\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import numpy as np\n",
    "import xarray\n",
    "from dask.diagnostics import ProgressBar\n",
    "import os\n",
    "#from tqdm.std import tqdm\n",
    "\n",
    "def parse_file_parts(file_name):\n",
    "  return dict(part.split(\"-\", 1) for part in file_name.split(\"_\"))\n",
    "\n",
    "\n",
    "dl_dir = '/g/data/wb00/admin/testing/graphcast/'\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb59ef-d8b9-49b9-b005-d0a609470ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "050c26a9-a703-4265-a2bc-4a5fb39781a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob: dataset/source-era5_date-2022-01-01_res-0.25_levels-13_steps-01.nc, Size: 0.98GB, Md5_hash: uue+jAi9QokoQ2A4ORbSDA==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-era5_date-2022-01-01_res-0.25_levels-13_steps-01.nc\n",
      "\n",
      "Blob: dataset/source-era5_date-2022-01-01_res-0.25_levels-13_steps-04.nc, Size: 1.96GB, Md5_hash: y1VEYF1Up43Rdjzibtzrsw==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-era5_date-2022-01-01_res-0.25_levels-13_steps-04.nc\n",
      "\n",
      "Blob: dataset/source-era5_date-2022-01-01_res-0.25_levels-13_steps-12.nc, Size: 4.56GB, Md5_hash: o4Alk7EXrcQeDCWkSfBl0A==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-era5_date-2022-01-01_res-0.25_levels-13_steps-12.nc\n",
      "\n",
      "Blob: dataset/source-era5_date-2022-01-01_res-0.25_levels-37_steps-01.nc, Size: 2.65GB, Md5_hash: tBIMMxpZKlmea4Cav6TB5Q==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-era5_date-2022-01-01_res-0.25_levels-37_steps-01.nc\n",
      "\n",
      "Blob: dataset/source-era5_date-2022-01-01_res-0.25_levels-37_steps-04.nc, Size: 5.3GB, Md5_hash: z7bbl/lZUdbFpOkLHcX+gw==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-era5_date-2022-01-01_res-0.25_levels-37_steps-04.nc\n",
      "\n",
      "Blob: dataset/source-era5_date-2022-01-01_res-0.25_levels-37_steps-12.nc, Size: 12.35GB, Md5_hash: eBO/PmBKQck7LTvkLM0uKQ==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-era5_date-2022-01-01_res-0.25_levels-37_steps-12.nc\n",
      "\n",
      "Blob: dataset/source-era5_date-2022-01-01_res-1.0_levels-13_steps-01.nc, Size: 0.06GB, Md5_hash: n3gQI37WTqcJy1FXrUYaUA==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-era5_date-2022-01-01_res-1.0_levels-13_steps-01.nc\n",
      "\n",
      "Blob: dataset/source-era5_date-2022-01-01_res-1.0_levels-13_steps-04.nc, Size: 0.12GB, Md5_hash: xhxpTl8BIkb0lF51GTxVRw==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-era5_date-2022-01-01_res-1.0_levels-13_steps-04.nc\n",
      "\n",
      "Blob: dataset/source-era5_date-2022-01-01_res-1.0_levels-13_steps-12.nc, Size: 0.29GB, Md5_hash: r1z84EeXQZYRPYjS6mMr7A==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-era5_date-2022-01-01_res-1.0_levels-13_steps-12.nc\n",
      "\n",
      "Blob: dataset/source-era5_date-2022-01-01_res-1.0_levels-13_steps-20.nc, Size: 0.45GB, Md5_hash: ruPTa+YVTmvWdrPqsgtuDw==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-era5_date-2022-01-01_res-1.0_levels-13_steps-20.nc\n",
      "\n",
      "Blob: dataset/source-era5_date-2022-01-01_res-1.0_levels-13_steps-40.nc, Size: 0.86GB, Md5_hash: 3K1KB3KI0Cii70aB7k0U1g==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-era5_date-2022-01-01_res-1.0_levels-13_steps-40.nc\n",
      "\n",
      "Blob: dataset/source-era5_date-2022-01-01_res-1.0_levels-37_steps-01.nc, Size: 0.17GB, Md5_hash: tWYCX5tby1BhK7iCuQ3MAg==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-era5_date-2022-01-01_res-1.0_levels-37_steps-01.nc\n",
      "\n",
      "Blob: dataset/source-era5_date-2022-01-01_res-1.0_levels-37_steps-04.nc, Size: 0.33GB, Md5_hash: LheXbwTURgwz2e4gvcR/+w==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-era5_date-2022-01-01_res-1.0_levels-37_steps-04.nc\n",
      "\n",
      "Blob: dataset/source-era5_date-2022-01-01_res-1.0_levels-37_steps-12.nc, Size: 0.78GB, Md5_hash: Be3IqjigetzB3guM6xOT1g==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-era5_date-2022-01-01_res-1.0_levels-37_steps-12.nc\n",
      "\n",
      "Blob: dataset/source-era5_date-2022-01-01_res-1.0_levels-37_steps-20.nc, Size: 1.22GB, Md5_hash: GdjPO21/4WteCmklmlG1Lg==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-era5_date-2022-01-01_res-1.0_levels-37_steps-20.nc\n",
      "\n",
      "Blob: dataset/source-era5_date-2022-01-01_res-1.0_levels-37_steps-40.nc, Size: 2.32GB, Md5_hash: 2lglNon9DZZKl5Ux+LmWhw==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-era5_date-2022-01-01_res-1.0_levels-37_steps-40.nc\n",
      "\n",
      "Blob: dataset/source-fake_date-2000-01-01_res-6.0_levels-13_steps-01.nc, Size: 0.0GB, Md5_hash: NLP1UlZBH5E7f4LZ7CVdDw==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-fake_date-2000-01-01_res-6.0_levels-13_steps-01.nc\n",
      "\n",
      "Blob: dataset/source-fake_date-2000-01-01_res-6.0_levels-13_steps-04.nc, Size: 0.0GB, Md5_hash: rU1BP1Q842D1QMpFfCDZcA==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-fake_date-2000-01-01_res-6.0_levels-13_steps-04.nc\n",
      "\n",
      "Blob: dataset/source-fake_date-2000-01-01_res-6.0_levels-13_steps-12.nc, Size: 0.01GB, Md5_hash: W/K6ZuVX7T/RB55gDdcC3Q==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-fake_date-2000-01-01_res-6.0_levels-13_steps-12.nc\n",
      "\n",
      "Blob: dataset/source-fake_date-2000-01-01_res-6.0_levels-13_steps-20.nc, Size: 0.01GB, Md5_hash: omKhdywNWFzdAgYmlMA7XQ==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-fake_date-2000-01-01_res-6.0_levels-13_steps-20.nc\n",
      "\n",
      "Blob: dataset/source-fake_date-2000-01-01_res-6.0_levels-13_steps-40.nc, Size: 0.02GB, Md5_hash: AVTKQYv4d2ldNGIhJ3WP3g==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-fake_date-2000-01-01_res-6.0_levels-13_steps-40.nc\n",
      "\n",
      "Blob: dataset/source-fake_date-2000-01-01_res-6.0_levels-37_steps-01.nc, Size: 0.0GB, Md5_hash: nZlwBIifidGOJRFk+7Fliw==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-fake_date-2000-01-01_res-6.0_levels-37_steps-01.nc\n",
      "\n",
      "Blob: dataset/source-fake_date-2000-01-01_res-6.0_levels-37_steps-04.nc, Size: 0.01GB, Md5_hash: UzKxndLgOBsAm6/X+KYKGA==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-fake_date-2000-01-01_res-6.0_levels-37_steps-04.nc\n",
      "\n",
      "Blob: dataset/source-fake_date-2000-01-01_res-6.0_levels-37_steps-12.nc, Size: 0.02GB, Md5_hash: WJ3PMZraNgsRD19pc3Xm2A==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-fake_date-2000-01-01_res-6.0_levels-37_steps-12.nc\n",
      "\n",
      "Blob: dataset/source-fake_date-2000-01-01_res-6.0_levels-37_steps-20.nc, Size: 0.03GB, Md5_hash: Hd4f9FwMV6pHS1Y5IKacVg==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-fake_date-2000-01-01_res-6.0_levels-37_steps-20.nc\n",
      "\n",
      "Blob: dataset/source-fake_date-2000-01-01_res-6.0_levels-37_steps-40.nc, Size: 0.07GB, Md5_hash: x89vc6PvKumPX1RQWXTLjA==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-fake_date-2000-01-01_res-6.0_levels-37_steps-40.nc\n",
      "\n",
      "Blob: dataset/source-hres_date-2022-01-01_res-0.25_levels-13_steps-01.nc, Size: 0.98GB, Md5_hash: CpfW9u84eIw0o+OfpiZr/w==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-hres_date-2022-01-01_res-0.25_levels-13_steps-01.nc\n",
      "\n",
      "Blob: dataset/source-hres_date-2022-01-01_res-0.25_levels-13_steps-04.nc, Size: 1.96GB, Md5_hash: XHXuiw4iq76k03mULrM56w==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-hres_date-2022-01-01_res-0.25_levels-13_steps-04.nc\n",
      "\n",
      "Blob: dataset/source-hres_date-2022-01-01_res-0.25_levels-13_steps-12.nc, Size: 4.56GB, Md5_hash: RENaPYOlIcMJ4J0bEs0/0A==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/dataset/source-hres_date-2022-01-01_res-0.25_levels-13_steps-12.nc\n",
      "\n",
      "Blob: dataset_$folder$, Size: 0.0GB, Md5_hash: hRFItP2P1650vZEAxcDImA==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast//dataset_$folder$\n",
      "\n",
      "Blob: params/GraphCast - ERA5 1979-2017 - resolution 0.25 - pressure levels 37 - mesh 2to6 - precipitation input and output.npz, Size: 0.14GB, Md5_hash: w+1bpwpEaR1SorPLZ3Gojw==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/params/GraphCast - ERA5 1979-2017 - resolution 0.25 - pressure levels 37 - mesh 2to6 - precipitation input and output.npz\n",
      "\n",
      "Blob: params/GraphCast_operational - ERA5-HRES 1979-2021 - resolution 0.25 - pressure levels 13 - mesh 2to6 - precipitation output only.npz, Size: 0.13GB, Md5_hash: Uhe1BhewAlUNjRqpDOUsEw==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/params/GraphCast_operational - ERA5-HRES 1979-2021 - resolution 0.25 - pressure levels 13 - mesh 2to6 - precipitation output only.npz\n",
      "\n",
      "Blob: params/GraphCast_small - ERA5 1979-2015 - resolution 1.0 - pressure levels 13 - mesh 2to5 - precipitation input and output.npz, Size: 0.13GB, Md5_hash: P1mpXrkp4g+QadUm53CrRQ==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/params/GraphCast_small - ERA5 1979-2015 - resolution 1.0 - pressure levels 13 - mesh 2to5 - precipitation input and output.npz\n",
      "\n",
      "Blob: params_$folder$, Size: 0.0GB, Md5_hash: hRFItP2P1650vZEAxcDImA==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast//params_$folder$\n",
      "\n",
      "Blob: stats/diffs_stddev_by_level.nc, Size: 0.0GB, Md5_hash: THJe+y35LXDTIEguPpmMig==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/stats/diffs_stddev_by_level.nc\n",
      "\n",
      "Blob: stats/mean_by_level.nc, Size: 0.0GB, Md5_hash: 7oRQdg7rjxivZKfgooIJ4A==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/stats/mean_by_level.nc\n",
      "\n",
      "Blob: stats/stddev_by_level.nc, Size: 0.0GB, Md5_hash: IufRTNuF8VoLEvuMiPfyuA==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast/stats/stddev_by_level.nc\n",
      "\n",
      "Blob: stats_$folder$, Size: 0.0GB, Md5_hash: hRFItP2P1650vZEAxcDImA==\n",
      "Destination file exists:  /g/data/wb00/admin/testing/graphcast//stats_$folder$\n",
      "\n",
      "CPU times: user 62.8 ms, sys: 151 ms, total: 214 ms\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "#os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"/scratch/fp0/mah900/graphcast/nci-2023-01-22c2d43b7789.json\"\n",
    "bucket_name = \"dm_graphcast\"\n",
    "\n",
    "dl_dir = '/g/data/wb00/admin/testing/graphcast/'\n",
    "storage_client = storage.Client.create_anonymous_client()\n",
    "bucket         = storage_client.get_bucket(\"dm_graphcast\")\n",
    "#storage_client = storage.Client()\n",
    "#blobs_list     = bucket.list_blobs(\"\"))\n",
    "#for file in file_list:\n",
    "#    print (file.name)\n",
    "file_list = storage_client.list_blobs(bucket_name)\n",
    "for blob in file_list:\n",
    "    print (\"Blob:\", blob.name+\",\", \"Size:\", str(round(blob.size/(2**30),2) )+\"GB,\", \"Md5_hash:\",  blob.md5_hash )\n",
    "    if blob.name.endswith(\"/\"):\n",
    "        continue\n",
    "    file_split = blob.name.split(\"/\")\n",
    "    directory  = \"/\".join(file_split[0:-1])\n",
    "    filename   = file_split[-1]\n",
    "    Path(dl_dir + directory).mkdir(parents=True, exist_ok=True)\n",
    "    destination_file_name = dl_dir + directory + \"/\" + filename\n",
    "    \n",
    "    if os.path.isfile(destination_file_name):\n",
    "        print(\"Destination file exists: \", destination_file_name)\n",
    "        print()\n",
    "        continue\n",
    "        \n",
    "    print(destination_file_name)\n",
    "    with open(destination_file_name, 'wb') as f:\n",
    "        with tqdm.wrapattr(f, \"write\", total=blob.size, miniters=blob.size/100) as file_obj:\n",
    "            storage_client.download_blob_to_file(blob, file_obj)\n",
    "    #blob.download_to_filename(dl_dir + directory + \"/\" + filename) \n",
    "    #print(hashlib.md5(open(destination_file_name,'rb').read()).hexdigest())\n",
    "    print()\n",
    "\n",
    "#gcs_client = storage.Client.create_anonymous_client()\n",
    "#gcs_bucket = gcs_client.get_bucket(\"dm_graphcast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a079ed20-9c9f-4adc-9d9c-c89adf478576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df670e39-54e4-4c79-bbbe-1887be22f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "\n",
    "def select(\n",
    "    data: xarray.Dataset,\n",
    "    variable: str,\n",
    "    level: Optional[int] = None,\n",
    "    max_steps: Optional[int] = None\n",
    "    ) -> xarray.Dataset:\n",
    "  data = data[variable]\n",
    "  if \"batch\" in data.dims:\n",
    "    data = data.isel(batch=0)\n",
    "  if max_steps is not None and \"time\" in data.sizes and max_steps < data.sizes[\"time\"]:\n",
    "    data = data.isel(time=range(0, max_steps))\n",
    "  if level is not None and \"level\" in data.coords:\n",
    "    data = data.sel(level=level)\n",
    "  return data\n",
    "\n",
    "def scale(\n",
    "    data: xarray.Dataset,\n",
    "    center: Optional[float] = None,\n",
    "    robust: bool = False,\n",
    "    ) -> tuple[xarray.Dataset, matplotlib.colors.Normalize, str]:\n",
    "  vmin = np.nanpercentile(data, (2 if robust else 0))\n",
    "  vmax = np.nanpercentile(data, (98 if robust else 100))\n",
    "  if center is not None:\n",
    "    diff = max(vmax - center, center - vmin)\n",
    "    vmin = center - diff\n",
    "    vmax = center + diff\n",
    "  return (data, matplotlib.colors.Normalize(vmin, vmax),\n",
    "          (\"RdBu_r\" if center is not None else \"viridis\"))\n",
    "\n",
    "def plot_data(\n",
    "    data: dict[str, xarray.Dataset],\n",
    "    fig_title: str,\n",
    "    plot_size: float = 5,\n",
    "    robust: bool = False,\n",
    "    cols: int = 4\n",
    "    ) -> tuple[xarray.Dataset, matplotlib.colors.Normalize, str]:\n",
    "\n",
    "  first_data = next(iter(data.values()))[0]\n",
    "  max_steps = first_data.sizes.get(\"time\", 1)\n",
    "  assert all(max_steps == d.sizes.get(\"time\", 1) for d, _, _ in data.values())\n",
    "\n",
    "  cols = min(cols, len(data))\n",
    "  rows = math.ceil(len(data) / cols)\n",
    "  figure = plt.figure(figsize=(plot_size * 2 * cols,\n",
    "                               plot_size * rows))\n",
    "  figure.suptitle(fig_title, fontsize=16)\n",
    "  figure.subplots_adjust(wspace=0, hspace=0)\n",
    "  figure.tight_layout()\n",
    "\n",
    "  images = []\n",
    "  for i, (title, (plot_data, norm, cmap)) in enumerate(data.items()):\n",
    "    ax = figure.add_subplot(rows, cols, i+1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(title)\n",
    "    im = ax.imshow(\n",
    "        plot_data.isel(time=0, missing_dims=\"ignore\"), norm=norm,\n",
    "        origin=\"lower\", cmap=cmap)\n",
    "    plt.colorbar(\n",
    "        mappable=im,\n",
    "        ax=ax,\n",
    "        orientation=\"vertical\",\n",
    "        pad=0.02,\n",
    "        aspect=16,\n",
    "        shrink=0.75,\n",
    "        cmap=cmap,\n",
    "        extend=(\"both\" if robust else \"neither\"))\n",
    "    images.append(im)\n",
    "\n",
    "  def update(frame):\n",
    "    if \"time\" in first_data.dims:\n",
    "      td = datetime.timedelta(microseconds=first_data[\"time\"][frame].item() / 1000)\n",
    "      figure.suptitle(f\"{fig_title}, {td}\", fontsize=16)\n",
    "    else:\n",
    "      figure.suptitle(fig_title, fontsize=16)\n",
    "    for im, (plot_data, norm, cmap) in zip(images, data.values()):\n",
    "      im.set_data(plot_data.isel(time=frame, missing_dims=\"ignore\"))\n",
    "\n",
    "  ani = animation.FuncAnimation(\n",
    "      fig=figure, func=update, frames=max_steps, interval=250)\n",
    "  plt.close(figure.number)\n",
    "  return HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e343137-e1ca-4092-b490-8adebba403db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d67b8fe-a61a-4b98-a460-58506c34f532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24780a7a45ab4fa8a65015844b1228df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(Dropdown(description='Params file:', layout=Layout(width='max-content'), options=â€¦"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Choose the model\n",
    "\n",
    "#params_file_options = [\n",
    "#    name for blob in gcs_bucket.list_blobs(prefix=\"params/\")\n",
    "#    if (name := blob.name.removeprefix(\"params/\"))]  # Drop empty string.\n",
    "params_file_options = [f for f in listdir(dl_dir+\"params/\") if isfile(join(dl_dir+\"params/\", f))]\n",
    "\n",
    "random_mesh_size = widgets.IntSlider(\n",
    "    value=4, min=4, max=6, description=\"Mesh size:\")\n",
    "random_gnn_msg_steps = widgets.IntSlider(\n",
    "    value=4, min=1, max=32, description=\"GNN message steps:\")\n",
    "random_latent_size = widgets.Dropdown(\n",
    "    options=[int(2**i) for i in range(4, 10)], value=32,description=\"Latent size:\")\n",
    "random_levels = widgets.Dropdown(\n",
    "    options=[13, 37], value=13, description=\"Pressure levels:\")\n",
    "\n",
    "\n",
    "params_file = widgets.Dropdown(\n",
    "    options=params_file_options,\n",
    "    description=\"Params file:\",\n",
    "    layout={\"width\": \"max-content\"})\n",
    "\n",
    "source_tab = widgets.Tab([\n",
    "    params_file,\n",
    "    widgets.VBox([\n",
    "        random_mesh_size,\n",
    "        random_gnn_msg_steps,\n",
    "        random_latent_size,\n",
    "        random_levels,\n",
    "    ]),\n",
    "    #params_file,\n",
    "])\n",
    "#source_tab.set_title(0, \"Random\")\n",
    "#source_tab.set_title(1, \"Checkpoint\")\n",
    "source_tab.set_title(1, \"Random\")\n",
    "source_tab.set_title(0, \"Checkpoint\")\n",
    "widgets.VBox([\n",
    "    source_tab,\n",
    "    widgets.Label(value=\"Run the next cell to load the model. Rerunning this cell clears your selection.\")\n",
    "])\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79eba1eb-d30f-4cbe-a732-99fcbd8e2a14",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# @title Load the model\n",
    "#from tqdm.std import tqdm\n",
    "source = source_tab.get_title(source_tab.selected_index)\n",
    "\n",
    "if source == \"Random\":\n",
    "  params = None  # Filled in below\n",
    "  state = {}\n",
    "  model_config = graphcast.ModelConfig(\n",
    "      resolution=0,\n",
    "      mesh_size=random_mesh_size.value,\n",
    "      latent_size=random_latent_size.value,\n",
    "      gnn_msg_steps=random_gnn_msg_steps.value,\n",
    "      hidden_layers=1,\n",
    "      radius_query_fraction_edge_length=0.6)\n",
    "  task_config = graphcast.TaskConfig(\n",
    "      input_variables=graphcast.TASK.input_variables,\n",
    "      target_variables=graphcast.TASK.target_variables,\n",
    "      forcing_variables=graphcast.TASK.forcing_variables,\n",
    "      pressure_levels=graphcast.PRESSURE_LEVELS[random_levels.value],\n",
    "      input_duration=graphcast.TASK.input_duration,\n",
    "  )\n",
    "else:\n",
    "  assert source == \"Checkpoint\"\n",
    "  #with gcs_bucket.blob(f\"params/{params_file.value}\").open(\"rb\") as f:\n",
    "  with open(dl_dir + \"params/\" + str(params_file.value), \"rb\") as f:\n",
    "      ckpt = checkpoint.load(f, graphcast.CheckPoint)\n",
    "      #with tqdm.wrapattr( f, \"write\", total=gcs_bucket.blob(f\"params/{params_file.value}\").size ) as file_obj:       \n",
    "      #    ckpt = checkpoint.load(file_obj, graphcast.CheckPoint)\n",
    "  params = ckpt.params\n",
    "  state = {}\n",
    "\n",
    "  model_config = ckpt.model_config\n",
    "  task_config = ckpt.task_config\n",
    "  print(\"Model description:\\n\", ckpt.description, \"\\n\")\n",
    "  print(\"Model license:\\n\", ckpt.license, \"\\n\")\n",
    "\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1dbe7d-ab4f-4405-87a5-bab593a358ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ae7c0-f00b-4831-b18a-58734425b9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48dfcbea-0ce1-45c9-9653-58428da24d0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:24\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_config' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# @title Get and filter the list of available example datasets\n",
    "\n",
    "#dataset_file_options = [\n",
    "#    name for blob in gcs_bucket.list_blobs(prefix=\"dataset/\")\n",
    "#    if (name := blob.name.removeprefix(\"dataset/\"))]  # Drop empty string.\n",
    "dataset_file_options = [f for f in listdir(dl_dir+\"dataset/\") if isfile(join(dl_dir+\"dataset/\", f))]\n",
    "\n",
    "def data_valid_for_model(\n",
    "    file_name: str, model_config: graphcast.ModelConfig, task_config: graphcast.TaskConfig):\n",
    "  file_parts = parse_file_parts(file_name.removesuffix(\".nc\"))\n",
    "  return (\n",
    "      model_config.resolution in (0, float(file_parts[\"res\"])) and\n",
    "      len(task_config.pressure_levels) == int(file_parts[\"levels\"]) and\n",
    "      (\n",
    "          (\"total_precipitation_6hr\" in task_config.input_variables and\n",
    "           file_parts[\"source\"] in (\"era5\", \"fake\")) or\n",
    "          (\"total_precipitation_6hr\" not in task_config.input_variables and\n",
    "           file_parts[\"source\"] in (\"hres\", \"fake\"))\n",
    "      )\n",
    "  )\n",
    "\n",
    "\n",
    "dataset_file = widgets.Dropdown(\n",
    "    options=[\n",
    "        (\", \".join([f\"{k}: {v}\" for k, v in parse_file_parts(option.removesuffix(\".nc\")).items()]), option)\n",
    "        for option in dataset_file_options\n",
    "        if data_valid_for_model(option, model_config, task_config)\n",
    "    ][::-1],\n",
    "    description=\"Dataset file:\",\n",
    "    layout={\"width\": \"max-content\"})\n",
    "widgets.VBox([\n",
    "    dataset_file,\n",
    "    widgets.Label(value=\"Run the next cell to load the dataset. Rerunning this cell clears your selection and refilters the datasets that match your model.\")\n",
    "])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f119906e-539e-4b2f-8d99-6fa07c403b96",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:4\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_file' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# @title Load weather data\n",
    "#from dask.diagnostics import ProgressBar\n",
    "\n",
    "if not data_valid_for_model(dataset_file.value, model_config, task_config):\n",
    "  raise ValueError(\n",
    "      \"Invalid dataset file, rerun the cell above and choose a valid dataset file.\")\n",
    "\n",
    "#with gcs_bucket.blob(f\"dataset/{dataset_file.value}\").open(\"rb\") as f:\n",
    "with open(dl_dir + \"dataset/\" + str(dataset_file.value), \"rb\") as f:    \n",
    "    #with ProgressBar():\n",
    "      example_batch = xarray.load_dataset(f).compute()\n",
    "\n",
    "assert example_batch.dims[\"time\"] >= 3  # 2 for input, >=1 for targets\n",
    "\n",
    "print(\", \".join([f\"{k}: {v}\" for k, v in parse_file_parts(dataset_file.value.removesuffix(\".nc\")).items()]))\n",
    "\n",
    "example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ea468b5-facb-4fae-8519-aeeb071b6302",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:4\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'example_batch' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# @title Choose data to plot\n",
    "\n",
    "plot_example_variable = widgets.Dropdown(\n",
    "    options=example_batch.data_vars.keys(),\n",
    "    value=\"2m_temperature\",\n",
    "    description=\"Variable\")\n",
    "plot_example_level = widgets.Dropdown(\n",
    "    options=example_batch.coords[\"level\"].values,\n",
    "    value=500,\n",
    "    description=\"Level\")\n",
    "plot_example_robust = widgets.Checkbox(value=True, description=\"Robust\")\n",
    "plot_example_max_steps = widgets.IntSlider(\n",
    "    min=1, max=example_batch.dims[\"time\"], value=example_batch.dims[\"time\"],\n",
    "    description=\"Max steps\")\n",
    "\n",
    "widgets.VBox([\n",
    "    plot_example_variable,\n",
    "    plot_example_level,\n",
    "    plot_example_robust,\n",
    "    plot_example_max_steps,\n",
    "    widgets.Label(value=\"Run the next cell to plot the data. Rerunning this cell clears your selection.\")\n",
    "])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40f252d1-8d83-4df0-9685-cb68ad68d77d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# @title Plot example data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m plot_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[1;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m: scale(select(\u001b[43mexample_batch\u001b[49m, plot_example_variable\u001b[38;5;241m.\u001b[39mvalue, plot_example_level\u001b[38;5;241m.\u001b[39mvalue, plot_example_max_steps\u001b[38;5;241m.\u001b[39mvalue),\n\u001b[1;32m      7\u001b[0m               robust\u001b[38;5;241m=\u001b[39mplot_example_robust\u001b[38;5;241m.\u001b[39mvalue),\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m fig_title \u001b[38;5;241m=\u001b[39m plot_example_variable\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m example_batch[plot_example_variable\u001b[38;5;241m.\u001b[39mvalue]\u001b[38;5;241m.\u001b[39mcoords:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'example_batch' is not defined"
     ]
    }
   ],
   "source": [
    "# @title Plot example data\n",
    "\n",
    "plot_size = 7\n",
    "\n",
    "data = {\n",
    "    \" \": scale(select(example_batch, plot_example_variable.value, plot_example_level.value, plot_example_max_steps.value),\n",
    "              robust=plot_example_robust.value),\n",
    "}\n",
    "fig_title = plot_example_variable.value\n",
    "if \"level\" in example_batch[plot_example_variable.value].coords:\n",
    "  fig_title += f\" at {plot_example_level.value} hPa\"\n",
    "\n",
    "plot_data(data, fig_title, plot_size, plot_example_robust.value)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c2b0f-7883-488a-a017-538d4e957ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Choose training and eval data to extract\n",
    "train_steps = widgets.IntSlider(\n",
    "    value=1, min=1, max=example_batch.sizes[\"time\"]-2, description=\"Train steps\")\n",
    "eval_steps = widgets.IntSlider(\n",
    "    value=example_batch.sizes[\"time\"]-2, min=1, max=example_batch.sizes[\"time\"]-2, description=\"Eval steps\")\n",
    "\n",
    "widgets.VBox([\n",
    "    train_steps,\n",
    "    eval_steps,\n",
    "    widgets.Label(value=\"Run the next cell to extract the data. Rerunning this cell clears your selection.\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6fb30d-02ff-4534-b85b-eb07eb72e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# @title Extract training and eval data\n",
    "\n",
    "train_inputs, train_targets, train_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "    example_batch, target_lead_times=slice(\"6h\", f\"{train_steps.value*6}h\"),\n",
    "    **dataclasses.asdict(task_config))\n",
    "\n",
    "eval_inputs, eval_targets, eval_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "    example_batch, target_lead_times=slice(\"6h\", f\"{eval_steps.value*6}h\"),\n",
    "    **dataclasses.asdict(task_config))\n",
    "\n",
    "print(\"All Examples:  \", example_batch.dims.mapping)\n",
    "print(\"Train Inputs:  \", train_inputs.dims.mapping)\n",
    "print(\"Train Targets: \", train_targets.dims.mapping)\n",
    "print(\"Train Forcings:\", train_forcings.dims.mapping)\n",
    "print(\"Eval Inputs:   \", eval_inputs.dims.mapping)\n",
    "print(\"Eval Targets:  \", eval_targets.dims.mapping)\n",
    "print(\"Eval Forcings: \", eval_forcings.dims.mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723cb383-226c-4c71-95f1-b0963d125fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# @title Load normalization data\n",
    "\n",
    "#with gcs_bucket.blob(\"stats/diffs_stddev_by_level.nc\").open(\"rb\") as f:\n",
    "#  diffs_stddev_by_level = xarray.load_dataset(f).compute()\n",
    "#with gcs_bucket.blob(\"stats/mean_by_level.nc\").open(\"rb\") as f:\n",
    "#  mean_by_level = xarray.load_dataset(f).compute()\n",
    "#with gcs_bucket.blob(\"stats/stddev_by_level.nc\").open(\"rb\") as f:\n",
    "#  stddev_by_level = xarray.load_dataset(f).compute()\n",
    " \n",
    "with open(dl_dir + \"stats/diffs_stddev_by_level.nc\", \"rb\") as f:      \n",
    "  diffs_stddev_by_level = xarray.load_dataset(f).compute()\n",
    "with open(dl_dir + \"stats/mean_by_level.nc\", \"rb\") as f:\n",
    "  mean_by_level = xarray.load_dataset(f).compute()\n",
    "with open(dl_dir + \"stats/stddev_by_level.nc\", \"rb\") as f:\n",
    "  stddev_by_level = xarray.load_dataset(f).compute()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f82878-ea10-4674-bd04-f75aa6c3624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# @title Build jitted functions, and possibly initialize random weights\n",
    "\n",
    "def construct_wrapped_graphcast(\n",
    "    model_config: graphcast.ModelConfig,\n",
    "    task_config: graphcast.TaskConfig):\n",
    "  \"\"\"Constructs and wraps the GraphCast Predictor.\"\"\"\n",
    "  # Deeper one-step predictor.\n",
    "  predictor = graphcast.GraphCast(model_config, task_config)\n",
    "\n",
    "  # Modify inputs/outputs to `graphcast.GraphCast` to handle conversion to\n",
    "  # from/to float32 to/from BFloat16.\n",
    "  predictor = casting.Bfloat16Cast(predictor)\n",
    "\n",
    "  # Modify inputs/outputs to `casting.Bfloat16Cast` so the casting to/from\n",
    "  # BFloat16 happens after applying normalization to the inputs/targets.\n",
    "  predictor = normalization.InputsAndResiduals(\n",
    "      predictor,\n",
    "      diffs_stddev_by_level=diffs_stddev_by_level,\n",
    "      mean_by_level=mean_by_level,\n",
    "      stddev_by_level=stddev_by_level)\n",
    "\n",
    "  # Wraps everything so the one-step model can produce trajectories.\n",
    "  predictor = autoregressive.Predictor(predictor, gradient_checkpointing=True)\n",
    "  return predictor\n",
    "\n",
    "\n",
    "@hk.transform_with_state\n",
    "def run_forward(model_config, task_config, inputs, targets_template, forcings):\n",
    "  predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "  return predictor(inputs, targets_template=targets_template, forcings=forcings)\n",
    "\n",
    "\n",
    "@hk.transform_with_state\n",
    "def loss_fn(model_config, task_config, inputs, targets, forcings):\n",
    "  predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "  loss, diagnostics = predictor.loss(inputs, targets, forcings)\n",
    "  return xarray_tree.map_structure(\n",
    "      lambda x: xarray_jax.unwrap_data(x.mean(), require_jax=True),\n",
    "      (loss, diagnostics))\n",
    "\n",
    "def grads_fn(params, state, model_config, task_config, inputs, targets, forcings):\n",
    "  def _aux(params, state, i, t, f):\n",
    "    (loss, diagnostics), next_state = loss_fn.apply(\n",
    "        params, state, jax.random.PRNGKey(0), model_config, task_config,\n",
    "        i, t, f)\n",
    "    return loss, (diagnostics, next_state)\n",
    "  (loss, (diagnostics, next_state)), grads = jax.value_and_grad(\n",
    "      _aux, has_aux=True)(params, state, inputs, targets, forcings)\n",
    "  return loss, diagnostics, next_state, grads\n",
    "\n",
    "# Jax doesn't seem to like passing configs as args through the jit. Passing it\n",
    "# in via partial (instead of capture by closure) forces jax to invalidate the\n",
    "# jit cache if you change configs.\n",
    "def with_configs(fn):\n",
    "  return functools.partial(\n",
    "      fn, model_config=model_config, task_config=task_config)\n",
    "\n",
    "# Always pass params and state, so the usage below are simpler\n",
    "def with_params(fn):\n",
    "  return functools.partial(fn, params=params, state=state)\n",
    "\n",
    "# Our models aren't stateful, so the state is always empty, so just return the\n",
    "# predictions. This is requiredy by our rollout code, and generally simpler.\n",
    "def drop_state(fn):\n",
    "  return lambda **kw: fn(**kw)[0]\n",
    "\n",
    "init_jitted = jax.jit(with_configs(run_forward.init))\n",
    "\n",
    "if params is None:\n",
    "  params, state = init_jitted(\n",
    "      rng=jax.random.PRNGKey(0),\n",
    "      inputs=train_inputs,\n",
    "      targets_template=train_targets,\n",
    "      forcings=train_forcings)\n",
    "\n",
    "loss_fn_jitted = drop_state(with_params(jax.jit(with_configs(loss_fn.apply))))\n",
    "grads_fn_jitted = with_params(jax.jit(with_configs(grads_fn)))\n",
    "run_forward_jitted = drop_state(with_params(jax.jit(with_configs(\n",
    "    run_forward.apply))))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55211186-7b88-4372-b672-6b6c79eb43d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd282e4-3b16-4848-9753-e941420dfdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# @title Autoregressive rollout (loop in python)\n",
    "\n",
    "\n",
    "assert model_config.resolution in (0, 360. / eval_inputs.sizes[\"lon\"]), (\n",
    "  \"Model resolution doesn't match the data resolution. You likely want to \"\n",
    "  \"re-filter the dataset list, and download the correct data.\")\n",
    "\n",
    "print(\"Inputs:  \", eval_inputs.dims.mapping)\n",
    "print(\"Targets: \", eval_targets.dims.mapping)\n",
    "print(\"Forcings:\", eval_forcings.dims.mapping)\n",
    "\n",
    "predictions = rollout.chunked_prediction(\n",
    "    run_forward_jitted,\n",
    "    rng=jax.random.PRNGKey(0),\n",
    "    inputs=eval_inputs,\n",
    "    targets_template=eval_targets * np.nan,\n",
    "    forcings=eval_forcings)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a7e4f4-96d0-4258-be37-12ab2761a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# @title Choose predictions to plot\n",
    "\n",
    "plot_pred_variable = widgets.Dropdown(\n",
    "    options=predictions.data_vars.keys(),\n",
    "    value=\"2m_temperature\",\n",
    "    description=\"Variable\")\n",
    "plot_pred_level = widgets.Dropdown(\n",
    "    options=predictions.coords[\"level\"].values,\n",
    "    value=500,\n",
    "    description=\"Level\")\n",
    "plot_pred_robust = widgets.Checkbox(value=True, description=\"Robust\")\n",
    "plot_pred_max_steps = widgets.IntSlider(\n",
    "    min=1,\n",
    "    max=predictions.dims[\"time\"],\n",
    "    value=predictions.dims[\"time\"],\n",
    "    description=\"Max steps\")\n",
    "\n",
    "widgets.VBox([\n",
    "    plot_pred_variable,\n",
    "    plot_pred_level,\n",
    "    plot_pred_robust,\n",
    "    plot_pred_max_steps,\n",
    "    widgets.Label(value=\"Run the next cell to plot the predictions. Rerunning this cell clears your selection.\")\n",
    "])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13639ec3-10c2-4dc9-83c2-a6d9e3c2f67c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# @title Plot predictions\n",
    "\n",
    "plot_size = 5 #5\n",
    "plot_max_steps = min(predictions.dims[\"time\"], plot_pred_max_steps.value)\n",
    "\n",
    "data = {\n",
    "    \"Targets\": scale(select(eval_targets, plot_pred_variable.value, plot_pred_level.value, plot_max_steps), robust=plot_pred_robust.value),\n",
    "    \"Predictions\": scale(select(predictions, plot_pred_variable.value, plot_pred_level.value, plot_max_steps), robust=plot_pred_robust.value),\n",
    "    \"Diff\": scale((select(eval_targets, plot_pred_variable.value, plot_pred_level.value, plot_max_steps) -\n",
    "                        select(predictions, plot_pred_variable.value, plot_pred_level.value, plot_max_steps)),\n",
    "                       robust=plot_pred_robust.value, center=0),\n",
    "}\n",
    "fig_title = plot_pred_variable.value\n",
    "if \"level\" in predictions[plot_pred_variable.value].coords:\n",
    "  fig_title += f\" at {plot_pred_level.value} hPa\"\n",
    "\n",
    "plot_data(data, fig_title, plot_size, plot_pred_robust.value, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9444b6c-6781-4415-80ec-6a0cdf1a8b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
